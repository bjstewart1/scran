<!--
%\VignetteIndexEntry{Food for the mind: using scran to perform basic analyses of single-cell RNA-seq data}
%\VignettePackage{BiocStyle}
%\VignetteEngine{knitr::knitr}
-->

```{r, echo=FALSE, results="hide", message=FALSE}
require(knitr)
opts_chunk$set(error=FALSE, message=FALSE, warning=FALSE)
```

```{r style, echo=FALSE, results='asis'}
BiocStyle::markdown()
```

```{r setup, echo=FALSE, message=FALSE}
library(scran)
register(SerialParam())
set.seed(100)
```

# Food for the mind: using scran to perform basic analyses of single-cell RNA-seq data
Package: `r Biocpkg("scran")` <br />
Author: Aaron Lun (alun@wehi.edu.au) <br />
Compilation date: `r Sys.Date()`

# Introduction

Single-cell RNA sequencing (scRNA-seq) is a widely used technique for profiling gene expression in individual cells.
This allows molecular biology to be studied at a resolution that cannot be matched by bulk sequencing of cell populations.
Common analyses include detection of highly variable and correlated genes across cells, or assignment of cells to cell cycle phases.
Cell-specific biases also need to be normalized in a manner that is robust to low counts and technical noise.
The `r Biocpkg("scran")` package implements methods to perform these analyses.
This vignette provides a brief description of each method and some toy examples for how they are used.

# Setting up the data

We start off with a count matrix where each row is a gene and each column is a cell.

```{r}
ngenes <- 10000
ncells <- 200
mu <- 2^runif(ngenes, 3, 10)
gene.counts <- matrix(rnbinom(ngenes*ncells, mu=mu, size=2), nrow=ngenes)
```

We add some arbitrary Ensembl gene IDs to give the impression that this is real (mouse) data.

```{r}
library(org.Mm.eg.db)
all.ensembl <- unique(toTable(org.Mm.egENSEMBL)$ensembl_id)
rownames(gene.counts) <- sample(all.ensembl, ngenes)
```

We also have a set of counts for spike-in transcripts.
These are appended to the counts for the endogenous genes.

```{r}
nspikes <- 100
ncells <- 200
mu <- 2^runif(nspikes, 3, 10)
spike.counts <- matrix(rnbinom(nspikes*ncells, mu=mu, size=2), nrow=nspikes)
rownames(spike.counts) <- paste0("ERCC-", seq_len(nspikes))
all.counts <- rbind(gene.counts, spike.counts)
```

Finally, we construct a `SCESet` object to store all of the data.
We also indicate which rows correspond to spike-in transcripts.
This is done through the `calculateQCMetrics` method from `r Biocpkg("scater")`, which takes a named list of sets of control genes.
We can then easily indicate which sets of controls are spike-ins using the `isSpike` setter function.
(In this case, there is only one control set, so the process may seem more complicated than necessary. 
The usefulness of this setup becomes more obvious when multiple control sets are present.)
This information can be easily extracted using the `isSpike` or `spikes` methods.

```{r}
library(scran)
sce <- newSCESet(countData=data.frame(all.counts))
sce <- calculateQCMetrics(sce, feature_controls=list(
    MySpikes=rep(c(FALSE, TRUE), c(ngenes, nspikes))
))
isSpike(sce) <- "MySpikes"
```

This is simulated data, so we assume that quality control has already been applied to remove low-quality cells or low-abundance genes.
Check out the `r Biocpkg("scater")` and `r Biocpkg("cellity")` packages for more details.

<!-- refer to the workflow for more details. -->

# Normalizing cell-specific biases

Cell-specific biases can be normalized using the `computeSumFactors` method, which implements the deconvolution strategy for scRNA-seq normalization.
This computes size factors that are used to scale the counts in each cell.
The assumption is that most genes are not differentially expressed (DE) between cells, such that any differences in expression across the majority of genes represents some technical bias that should be removed.

```{r}
sce <- computeSumFactors(sce)
summary(sizeFactors(sce))
```

For larger data sets, clustering can be performed with the `quickCluster` function before normalization.
Briefly, cells are grouped into clusters of similar expression; normalization is applied within each cluster to compute size factors for each cell; and the factors are rescaled by normalization between clusters.
This reduces the risk of violating the above assumption when many genes are DE across a heterogeneous population.

```{r}
larger.sce <- newSCESet(countData=data.frame(cbind(all.counts, all.counts, all.counts)))
clusters <- quickCluster(larger.sce)
larger.sce <- computeSumFactors(larger.sce, cluster=clusters)
```

An alternative approach is to normalize based on the spike-in counts.
The idea is that the same quantity of spike-in RNA was added to each cell prior to library preparation.
Size factors can then be computed to scale the counts such that the total coverage of the spike-in transcripts is equal across cells.
The main practical difference is that spike-in normalization preserves differences in total RNA content between cells, whereas `computeSumFactors` and other non-DE methods do not.

```{r}
sce2 <- computeSpikeFactors(sce)
summary(sizeFactors(sce2))
```

Normalized expression values can be calculated using the `normalize` method from `r Biocpkg("scater")`.
Note, however, that if the deconvolution size factors are used, then it is advisable to normalize the spike-ins separately.
This is because the spike-ins do not include the effect of total mRNA content, such that the deconvolution size factors will over-normalize the spike-in counts.
To avoid this, we set spike-in size factors to normalize the spike-ins separately, as shown below.
(Obviously, if the spike-in size factors were already being used for normalization, then this extra step is unnecessary.)

```{r}
sizeFactors(sce, type="MySpikes") <- computeSpikeFactors(sce, sf.out=TRUE)
sce <- normalize(sce)
```

# Cell cycle phase assignment

We use a pre-defined classifier to assign cells into their cell cycle phases.
This classifier was constructed from a training data set by identifying pairs of genes where the difference in expression within each pair changed sign across phases.
Thus, by examining the sign of the difference in test data, the phase to which the cell belongs can be identified.
Classifiers for human and mouse data are provided with the package -- for other systems, classifiers can be constructed from a training set using the `sandbag` function.

```{r}
mm.pairs <- readRDS(system.file("exdata", "mouse_cycle_markers.rds", package="scran"))
```

The classification itself is done using the `cyclone` function, given the count data and the trained classifier.
This yields a number of scores representing the consistency of the signs with each phase.

```{r}
assigned <- cyclone(sce, pairs=mm.pairs)
head(assigned$scores)
```

Cells are considered to be in G1 phase, if the G1 score is above 0.5 and the G2/M score is below 0.5;
    to be in G2/M phase, if the G2/M score is above 0.5 and the G1 score is below 0.5;
    to be in S phase, if both scores are below 0.5; and to be unknown, if both scores are above 0.5.
Despite the availability of a S score, it tends to be more accurate to assign cells based on the G1 and G2/M scores only.

```{r}
phase <- rep("S", ncol(sce))
phase[assigned$scores$G1 > 0.5] <- "G1"
phase[assigned$scores$G2M > 0.5] <- "G2M"
phase[assigned$scores$G1 > 0.5 & assigned$scores$G2M > 0.5] <- "unknown"
table(phase)
```

# Detecting highly variable genes

Highly variable genes (HVGs) are detected by decomposing the total variance of each gene into its biological and technical components.
This avoids prioritizing low-abundance genes that have large variances due to technical noise.
First, we fit a mean-variance trend to the normalized log-expression values with `trendVar`.
By default, this done using only the spike-in transcripts, as these should only exhibit technical noise.

```{r}
fit <- trendVar(sce)
```

The fitted value of the trend can then be used as an estimate of the technical component.
We subtract the fitted value from the total variance to obtain the biological component for each gene.
HVGs can be defined as the top set of genes with the largest biological components.

```{r}
decomp <- decomposeVar(sce, fit)
top.hvgs <- order(decomp$bio, decreasing=TRUE)
head(decomp[top.hvgs,])
```

This can be examined more visually by constructing a mean-variance plot.
Here, the black points represent the endogenous genes; the red points represent spike-in transcripts; and the red line represents the mean-variance trend fitted to the spike-ins.


```{r}
plot(decomp$mean, decomp$total, xlab="Mean log-expression", ylab="Variance")
o <- order(decomp$mean)
lines(decomp$mean[o], decomp$tech[o], col="red", lwd=2)
points(fit$mean, fit$var, col="red", pch=16)
```

If spike-ins are absent or of poor quality, an alternative is to fit the trend to the gene variances directly with `use.spikes=FALSE`.
This assumes that technical noise is the major contributor to the variance of most genes in the data set, such that the trend still represents the technical component.
The resulting fit can then be used in `decomposeVar` as described above.

```{r}
alt.fit <- trendVar(sce, use.spikes=FALSE) 
alt.decomp <- decomposeVar(sce, alt.fit)
```

If the data set already contains some uninteresting substructure (e.g., batch effects), we can block on this by setting the `design` argument in `trendVar`.
This ensures that the substructure does not inflate the variance estimates.
For example, if the cells were prepared in two separate batches, we could construct a design matrix incorporating this information with `model.matrix` and pass it to `trendVar`.
The same design will also be used in `decomposeVar`.

```{r}
batch <- rep(c("1", "2"), each=100)
design <- model.matrix(~batch)
alt.fit2 <- trendVar(sce, design=design)
alt.decomp2 <- decomposeVar(sce, alt.fit)
```

<!--
We use log-transformed values as the count-based models put too much weight on large outliers.
Consider the example below, where `counter1` contains an outlier but is otherwise less variable than the other `counter*` sets.
If you run the code, you'll find that `counter1` still has a larger NB dispersion (and by implication, CV^2^) and random effect variance than the others.
Only the variance of the log-counts does anything close to the right thing here.

```{r, eval=FALSE}
set.seed(102251)
ncells <- 100
niters <- 20
averaged.GLM <- averaged.GLMM <- averaged.log <- 0
for (it in seq_len(niters)) {

counter1 <- rnbinom(ncells, mu=5, size=1)
counter1[1] <- 10000
counter2 <- rnbinom(ncells, mu=5, size=0.2) # Arguably, all more variable than above.
counter3 <- rnbinom(ncells, mu=20, size=0.2)
counter4 <- rnbinom(ncells, mu=100, size=0.2)

# With GLMs.
require(edgeR)
y <- DGEList(rbind(counter1, counter2, counter3, counter4), lib.size=rep(1e6, ncells))
y <- estimateDisp(y, cbind(rep(1, ncells)), prior.df=0)
averaged.GLM <- averaged.GLM + y$tagwise.dispersion

# With GLMMs (random effect variance is logged).
require(lme4)
plate <- integer(ncells)
plate[(ncells/2+1):ncells] <- 1 # Just to stop it complaining.
averaged.GLMM <- averaged.GLMM + c(
    unlist(VarCorr(glmer(counter1 ~ (1|plate), family=negative.binomial(1)))),
    unlist(VarCorr(glmer(counter2 ~ (1|plate), family=negative.binomial(1)))),
    unlist(VarCorr(glmer(counter3 ~ (1|plate), family=negative.binomial(1)))),
    unlist(VarCorr(glmer(counter4 ~ (1|plate), family=negative.binomial(1)))))

# With variances of log-values.
averaged.log <- averaged.log + apply(cpm(y, log=TRUE), 1, var)

}
averaged.GLM/niters
averaged.GLMM/niters
averaged.log/niters
```
-->

# Detecting correlated genes

The top set of HVGs can be used to identify significant correlations between pairs of genes.
The idea is to distinguish between HVGs caused by random stochasticity, and those that are driving systematic heterogeneity, e.g., between subpopulations.
Correlations are computed in the `correlatePairs` method using a slightly modified version of Spearman's rho.
Testing is performed against the null hypothesis of independent genes, using a permutation method in `correlateNull` to construct a null distribution.

```{r}
null.dist <- correlateNull(ncol(sce))
cor.pairs <- correlatePairs(sce[top.hvgs[1:200],], null.dist=null.dist)
head(cor.pairs)
```

As with variance estimation, if uninteresting substructure is present, this should be blocked on using the `design` argument in both `correlateNull` and `correlatePairs`.

```{r}
null.dist2 <- correlateNull(design=design, iter=1e5) # fewer iterations, to speed it up.
cor.pairs2 <- correlatePairs(sce[top.hvgs[1:200],], null.dist=null.dist2, design=design)
```

Significant correlations between pairs of genes can be defined at a false discovery rate (FDR) threshold of, e.g., 5%.
In this case, no correlations are significant as the counts were randomly generated for each gene.
In other situations when correlated gene pairs are present, these can be used to construct heatmaps to verify whether subpopulations exist; for choosing marker genes in experimental validation; and to construct gene-gene association networks.

<!--
The modification simplifies things by allowing the same tie-free null distribution to be used for all genes.
It also means that we can get spuriously large correlations (which we would have been protected from, had we considered ties).
However, this is acceptable as we can let the error-control machinery deal with the possibility of such spuriously large values.
We also don't have to account for HVG identification in multiple testing here, because correlations are independent of the variance of the genes.

```{r, eval=FALSE}
set.seed(1023423)
ncells <- 100
null.dist <- correlateNull(ncells)
all.p <- list()
for (it in 1:10000) {
    x1 <- rpois(ncells, lambda=10)
    x2 <- rpois(ncells, lambda=20)
    rho2 <- cor(rank(x1, ties.method="random"), rank(x2, ties.method="random"), method="spearman")
    all.p[[it]] <- sum(null.dist >= rho2)/length(null.dist)
}
sum(unlist(all.p) <= 0.01)/10000
sum(unlist(all.p) <= 0.05)/10000
```

The idea is to mitigate dependence on explicit subpopulation identification for follow-up studies.
We identify strongly correlated genes first, then only need to check for subpopulations as a diagnostic.
This reduces the sensitivity of the analysis to ambiguity/uncertainty during subpopulation identification.
It's possible because validation requires genes, not subpopulations (as the current cells are destroyed), so we skip the middleman.
-->

# Converting to other formats

The `SCESet` object can be easily converted into other formats using the `convertTo` method.
This allows analyses to be performed using other pipelines and packages.
For example, if DE analyses were to be performed using `r Biocpkg("edgeR")`, the count data in `sce` could be used to construct a `DGEList`.

```{r}
y <- convertTo(sce, type="edgeR")
```

By default, rows corresponding to spike-in transcripts are dropped when `get.spikes=FALSE`.
As such, the rows of `y` may not correspond directly to the rows of `sce` -- users should match by row name to ensure correct cross-referencing between objects.
Normalization factors are also automatically computed from the size factors.

The same conversion strategy roughly applies to the other supported formats.
DE analyses can be performed using `r Biocpkg("DESeq2")` by converting the object to a `DESeqDataSet`.
Cells can be ordered on pseudotime with `r Biocpkg("monocle")` by converting the object to a `CellDataSet` (in this case, normalized _unlogged_ expression values are stored). 

# Summary

This vignette describes the main functions in the `r Biocpkg("scran")` package for basic analysis of single-cell RNA-seq data.
We cover normalization, cell cycle phase assignment, HVG detection and correlation testing.
Conversion to other formats can also be performed in preparation for analyses with other packages in the Bioconductor project.
Further information can be obtained by reading the documentation for each function (e.g., `?convertTo`), or asking for help on the Bioconductor [support site](http://support.bioconductor.org) (please read the [posting guide](http://www.bioconductor.org/help/support/posting-guide) beforehand).

<!-- We suggest reading the workflow, but I can't say that yet. -->

# Session information

```{r}
sessionInfo()
```
